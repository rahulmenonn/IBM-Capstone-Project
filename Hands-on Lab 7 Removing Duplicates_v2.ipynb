{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Removing Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will focus on data wrangling, an important step in preparing data for analysis. Data wrangling involves cleaning and organizing data to make it suitable for analysis. One key task in this process is removing duplicate entries, which are repeated entries that can distort analysis and lead to inaccurate conclusions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will perform the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify duplicate rows  in the dataset.\n",
    "2. Use suitable techniques to remove duplicate rows and verify the removal.\n",
    "3. Summarize how to handle missing values appropriately.\n",
    "4. Use ConvertedCompYearly to normalize compensation data.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset into a DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the dataset using pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the dataset\n",
    "file_path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you are working on a local Jupyter environment, you can use the URL directly in the <code>pandas.read_csv()</code>  function as shown below:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#df = pd.read_csv(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Identifying Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Identify Duplicate Rows**\n",
    "  1. Count the number of duplicate rows in the dataset.\n",
    "  2. Display the first few duplicate rows to understand their structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total exact duplicate rows: 0\n",
      "No exact duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Identify duplicate rows\n",
    "total_duplicates = df.duplicated().sum()\n",
    "print(\"Total exact duplicate rows:\", total_duplicates)\n",
    "\n",
    "if total_duplicates > 0:\n",
    "    dup_rows = df[df.duplicated(keep='first')]\n",
    "    print(\"\\nShowing first 10 duplicate rows:\")\n",
    "    display(dup_rows.head(10))\n",
    "else:\n",
    "    print(\"No exact duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Removing Duplicate Rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Remove Duplicates**\n",
    "   1. Remove duplicate rows from the dataset using the drop_duplicates() function.\n",
    "2. Verify the removal by counting the number of duplicate rows after removal .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate rows. New shape: (65437, 114)\n",
      "Duplicates remaining (should be 0): 0\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Remove duplicate rows\n",
    "df_no_dup = df.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "removed_count = df.shape[0] - df_no_dup.shape[0]\n",
    "print(f\"Removed {removed_count} duplicate rows. New shape: {df_no_dup.shape}\")\n",
    "\n",
    "# verify\n",
    "print(\"Duplicates remaining (should be 0):\", df_no_dup.duplicated().sum())\n",
    "\n",
    "# replace working df with deduped version (optional)\n",
    "df = df_no_dup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Identify and Handle Missing Values**\n",
    "   1. Identify missing values for all columns in the dataset.\n",
    "   2. Choose a column with significant missing values (e.g., EdLevel) and impute with the most frequent value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts (descending):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AINextMuch less integrated    64289\n",
       "AINextLess integrated         63082\n",
       "AINextNo change               52939\n",
       "AINextMuch more integrated    51999\n",
       "EmbeddedAdmired               48704\n",
       "                              ...  \n",
       "YearsCode                      5568\n",
       "NEWSOSites                     5151\n",
       "LearnCode                      4949\n",
       "EdLevel                        4653\n",
       "AISelect                       4530\n",
       "Length: 109, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column chosen for imputation: EdLevel\n",
      "Imputed EdLevel missing values with mode = 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)'. Before: 4653, After: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300/2128675424.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col_to_impute].fillna(mode_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Handling missing values\n",
    "print(\"Missing counts (descending):\")\n",
    "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "display(missing_counts[missing_counts > 0])\n",
    "\n",
    "# choose column to impute: prefer 'EdLevel'\n",
    "col_to_impute = 'EdLevel' if 'EdLevel' in df.columns else (missing_counts.idxmax() if missing_counts.max() > 0 else None)\n",
    "\n",
    "if col_to_impute is None:\n",
    "    print(\"No column with missing values to impute.\")\n",
    "else:\n",
    "    print(\"Column chosen for imputation:\", col_to_impute)\n",
    "    mode_vals = df[col_to_impute].mode(dropna=True)\n",
    "    if mode_vals.empty:\n",
    "        print(\"No mode available for this column (all values may be NaN). Skipping imputation.\")\n",
    "    else:\n",
    "        mode_val = mode_vals.iloc[0]\n",
    "        before = df[col_to_impute].isnull().sum()\n",
    "        df[col_to_impute].fillna(mode_val, inplace=True)\n",
    "        after = df[col_to_impute].isnull().sum()\n",
    "        print(f\"Imputed {col_to_impute} missing values with mode = {mode_val!r}. Before: {before}, After: {after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Normalizing Compensation Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Normalize Compensation Data Using ConvertedCompYearly**\n",
    "   1. Use the ConvertedCompYearly column for compensation analysis as the normalized annual compensation is already provided.\n",
    "   2. Check for missing values in ConvertedCompYearly and handle them if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvertedCompYearly missing after coercion: 42002\n",
      "Median ConvertedCompYearly: 65000.0\n",
      "Filled missing ConvertedCompYearly with median.\n",
      "\n",
      "ConvertedCompYearly summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_300/1171113345.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['ConvertedCompYearly'].fillna(median_val, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    6.543700e+04\n",
       "mean     7.257636e+04\n",
       "std      1.122207e+05\n",
       "min      1.000000e+00\n",
       "25%      6.500000e+04\n",
       "50%      6.500000e+04\n",
       "75%      6.500000e+04\n",
       "max      1.625660e+07\n",
       "Name: ConvertedCompYearly, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Normalize/check ConvertedCompYearly\n",
    "if 'ConvertedCompYearly' in df.columns:\n",
    "    df['ConvertedCompYearly'] = pd.to_numeric(df['ConvertedCompYearly'], errors='coerce')\n",
    "    missing_after_coerce = df['ConvertedCompYearly'].isnull().sum()\n",
    "    print(\"ConvertedCompYearly missing after coercion:\", missing_after_coerce)\n",
    "    median_val = df['ConvertedCompYearly'].median(skipna=True)\n",
    "    print(\"Median ConvertedCompYearly:\", median_val)\n",
    "    if pd.notna(median_val):\n",
    "        df['ConvertedCompYearly'].fillna(median_val, inplace=True)\n",
    "        print(\"Filled missing ConvertedCompYearly with median.\")\n",
    "    print(\"\\nConvertedCompYearly summary:\")\n",
    "    display(df['ConvertedCompYearly'].describe())\n",
    "else:\n",
    "    print(\"Column 'ConvertedCompYearly' not found — skipping normalization step.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Summary and Next Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this lab, you focused on identifying and removing duplicate rows.**\n",
    "\n",
    "- You handled missing values by imputing the most frequent value in a chosen column.\n",
    "\n",
    "- You used ConvertedCompYearly for compensation normalization and handled missing values.\n",
    "\n",
    "- For further analysis, consider exploring other columns or visualizing the cleaned dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Summary & verification\n",
    "print(\"Final dataframe shape:\", df.shape)\n",
    "print(\"Exact duplicates remaining:\", df.duplicated().sum())\n",
    "\n",
    "print(\"\\nColumns with remaining missing values (if any):\")\n",
    "remaining_missing = df.isnull().sum().sort_values(ascending=False)\n",
    "display(remaining_missing[remaining_missing > 0])\n",
    "\n",
    "print(\"\\nQuick checks you can run next:\")\n",
    "print(\" - df.head()\")\n",
    "print(\" - df.info()\")\n",
    "print(\" - df.describe(include='all')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-11-05|1.2|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-24|1.1|Madhusudhan Moole|Updated lab|\n",
    "|2024-09-23|1.0|Raghul Ramesh|Created lab|\n",
    "\n",
    "--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "2116052544ce403759eef2159eb3d21f1d38e895d652bcaffa36a5791482361d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
